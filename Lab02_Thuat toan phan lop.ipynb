{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "161fe8b2-612c-4f11-a6d7-811361621cd1",
   "metadata": {},
   "source": [
    "## **THỰC HÀNH: CÁC GIẢI THUẬT PHÂN LOẠI CƠ BẢN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e611711-7d38-447e-972b-7a95f26dc012",
   "metadata": {},
   "source": [
    "### **2.2. GIẢI THUẬT 2: SUPPORT VECTOR MACHINE (SVM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9583199b-9caf-4c95-a0f8-68f405ac3a52",
   "metadata": {},
   "source": [
    "#### **2.2.1. Ôn tập lý thuyết**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791c9be-c114-467e-9ebf-71260a418ec9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ***Sự khác biệt giữa SVM với lề cứng (hard margin) và lề mềm (soft margin) là gì? Khi nào nên sử dụng lề mềm?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992820b-1b8c-4f23-8ab5-ed55dc4e8c88",
   "metadata": {},
   "source": [
    "Sự khác biệt giữa SVM với lề cứng (hard margin) và lề mềm (soft margin) chủ yếu nằm ở cách xử lý dữ liệu trong những điều kiện khác nhau. Với lề cứng, mô hình giả định dữ liệu có thể phân tách tuyến tính hoàn hảo và yêu cầu tất cả các điểm huấn luyện đều nằm ngoài hoặc trên ranh giới lề. Cách tiếp cận này phù hợp với dữ liệu “sạch”, không nhiễu nhưng dễ thất bại khi xuất hiện ngoại lai hoặc dữ liệu không khả phân tuyến tính.\n",
    "\n",
    "Ngược lại, lề mềm được xây dựng để khắc phục hạn chế đó bằng cách cho phép một số điểm dữ liệu vi phạm, tức là có thể nằm trong vùng lề hoặc bị phân loại sai. Thông qua biến slack và tham số phạt C, mô hình cân bằng giữa việc mở rộng khoảng cách phân tách và giảm thiểu lỗi phân loại. \n",
    "\n",
    "Nhờ sự linh hoạt này, lề mềm xử lý tốt hơn trong thực tế, đặc biệt khi dữ liệu có nhiễu hoặc không thể phân tách tuyến tính hoàn toàn. Ngay cả khi dữ liệu phân tách tuyến tính nhưng lề quá hẹp khiến mô hình dễ overfitting, lề mềm vẫn giúp mở rộng lề và cải thiện khả năng khái quát hóa. Tóm lại, lề cứng thích hợp cho dữ liệu sạch, còn lề mềm phù hợp hơn trong hầu hết các bài toán thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d868dc0c-a763-48fc-9ead-78a0a90d63d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### ***Hàm nhân (kernel) trong SVM là gì? Hãy giải thích các loại kernel phổ biến (linear, polynomial, RBF) và khi nào nên sử dụng chúng.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ea632-ce29-4d2f-9c8b-bae084006252",
   "metadata": {},
   "source": [
    "Hàm nhân (Kernel Function) trong Support Vector Machine (SVM) là một công cụ toán học mạnh mẽ giúp mở rộng khả năng của SVM từ bài toán phân loại tuyến tính sang phân loại phi tuyến. Thay vì tìm siêu phẳng phân tách trực tiếp trong không gian đặc trưng ban đầu, kernel cho phép ánh xạ dữ liệu sang một không gian nhiều chiều hơn, nơi dữ liệu vốn không tuyến tính có thể trở nên tuyến tính và dễ phân tách hơn.\n",
    "\n",
    "Điểm đặc biệt nằm ở kernel trick là thay vì thực hiện ánh xạ và tính toán trực tiếp trong không gian cao chiều thì kernel chỉ cần tính tích vô hướng giữa các vector thông qua một hàm trong không gian gốc. Nhờ vậy, kernel giúp mô hình thu được thông tin về độ tương đồng của các điểm dữ liệu trong không gian cao chiều mà không cần biểu diễn tường minh dữ liệu ở đó.\n",
    "\n",
    "Về bản chất, kernel là một hàm nhận đầu vào là hai vector và trả về một giá trị vô hướng phản ánh mức độ tương đồng giữa hai vector trong không gian cao chiều. Vì vậy, kernel đóng vai trò như “người gác cổng” của không gian phức tạp, cho phép SVM tận dụng ưu thế của không gian cao chiều để phân loại mà không phải trả chi phí tính toán trực tiếp. Trong thực tế, Kernel SVM thường được sử dụng cho các tập dữ liệu không tuyến tính với những hàm nhân phổ biến như: kernel tuyến tính (Linear), đa thức (Polynomial) và Gaussian/RBF (Radial Basis Function).\n",
    "\n",
    "**Linear Kernel** là loại hàm nhân đơn giản và trực quan nhất, chỉ thực hiện phép tính tích vô hướng giữa hai vector trong không gian gốc mà không cần ánh xạ sang không gian cao hơn. Kernel này đặc biệt phù hợp khi dữ liệu có thể phân tách tuyến tính hoặc gần tuyến tính. Trong thực tế, Linear kernel thường được áp dụng cho dữ liệu có số chiều rất lớn, chẳng hạn như dữ liệu văn bản (Bag of Words, TF-IDF), nơi việc phân tách tuyến tính thường đã mang lại kết quả tốt. Ưu điểm của Linear kernel là tính toán nhanh, mô hình đơn giản và ít nguy cơ overfitting. Tuy nhiên, hạn chế của nó là không phù hợp với các bài toán có dữ liệu phức tạp và phi tuyến.\n",
    "\n",
    "**Polynomial Kernel** (Hàm nhân đa thức) cho phép mở rộng tích vô hướng thành dạng đa thức bậc d, từ đó mô hình có thể nắm bắt các mối quan hệ phi tuyến và sự tương tác phức tạp giữa các đặc trưng, chẳng hạn như quan hệ bậc hai hay bậc ba. Kernel này mang lại sự linh hoạt trong việc xây dựng ranh giới phân loại cho dữ liệu không tuyến tính, nhưng cũng đi kèm với chi phí tính toán cao hơn so với linear kernel. Ngoài ra, nếu bậc đa thức được chọn quá lớn, mô hình có thể trở nên quá phức tạp và dễ dẫn đến hiện tượng overfitting.\n",
    "\n",
    "**Gaussian / RBF (Radial Basis Function) Kernel** là một trong những kernel phổ biến và mạnh mẽ nhất. Kernel này ánh xạ dữ liệu sang một không gian vô hạn chiều, trong đó mỗi điểm được biểu diễn dựa trên khoảng cách tới các điểm khác, nhờ vậy có thể mô tả được những ranh giới phân tách cực kỳ phức tạp và phi tuyến. Với tính linh hoạt cao, RBF kernel được ứng dụng rộng rãi trong nhiều bài toán thực tế, đặc biệt khi dữ liệu khó phân tách tuyến tính. Tuy nhiên, nhược điểm của nó là đòi hỏi phải điều chỉnh cẩn thận các tham số như γ và C để tránh overfitting, đồng thời có thể tốn kém về tính toán khi làm việc với tập dữ liệu lớn.\n",
    "\n",
    "Việc chọn kernel phụ thuộc vào đặc điểm dữ liệu, mục tiêu phân loại và nguồn lực tính toán. Linear kernel phù hợp cho dữ liệu có thể phân tách tuyến tính hoặc dữ liệu nhiều chiều như văn bản. Polynomial kernel thích hợp khi dữ liệu có quan hệ đa thức bậc thấp đến trung bình. RBF kernel an toàn và linh hoạt cho hầu hết trường hợp dữ liệu phi tuyến phức tạp, nhưng tốn tài nguyên hơn. Trong thực tế, cách tiếp cận hiệu quả nhất là thử nghiệm nhiều kernel khác nhau và sử dụng cross-validation để tìm ra lựa chọn tối ưu cho từng bài toán cụ thể."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124513d-ee1d-4415-84b9-78ae8fabfb4c",
   "metadata": {},
   "source": [
    "#### ***Tham số C trong SVM có ý nghĩa gì? Nó ảnh hưởng như thế nào đến hiệu suất của mô hình?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317effd-30dd-4947-b2f7-4867b734c166",
   "metadata": {},
   "source": [
    "Tham số C trong SVM là một siêu tham số quan trọng, quyết định sự đánh đổi giữa độ rộng của biên (margin) và lỗi phân loại trên tập huấn luyện. Hiểu một cách đơn giản, C quy định mức phạt mà mô hình phải trả cho các điểm dữ liệu bị phân loại sai.\n",
    "\n",
    "Cụ thể, khi C nhỏ, mô hình cho phép tồn tại nhiều điểm sai hơn để đổi lại một biên rộng hơn. Điều này giúp tăng khả năng tổng quát, giảm nguy cơ overfitting, nhưng có thể làm tăng sai số huấn luyện. Ngược lại, khi C lớn, mô hình tập trung tối đa vào việc giảm lỗi huấn luyện, dẫn đến biên hẹp hơn và ít chấp nhận sai sót. Tuy nhiên, cách tiếp cận này dễ khiến mô hình quá khớp với dữ liệu huấn luyện và kém hiệu quả khi áp dụng cho dữ liệu mới.\n",
    "\n",
    "Về mặt hiệu suất mô hình:\n",
    "- Nếu C quá nhỏ → mô hình có nguy cơ underfitting do quá “dễ dãi” với lỗi phân loại.\n",
    "- Nếu C quá lớn → mô hình dễ overfitting do quá tập trung giảm sai số huấn luyện.\n",
    "\n",
    "Có thể nói, tham số C tỷ lệ nghịch với kích thước của biên: C càng lớn thì biên càng hẹp, C càng nhỏ thì biên càng rộng. Tham số này có thể áp dụng với bất kỳ loại kernel nào và thường được coi như một dạng tham số điều chuẩn (regularization), giúp mô hình cân bằng giữa độ chính xác trên dữ liệu huấn luyện và khả năng khái quát hóa. Ví dụ, với SVM sử dụng linear kernel và C = 1.0 (một giá trị tương đối lớn), mô hình sẽ tạo ra biên hẹp để giảm thiểu sai số huấn luyện.\n",
    "\n",
    "Trong thực tế, việc lựa chọn giá trị C tối ưu không cố định mà thường được xác định thông qua cross-validation. Cách tiếp cận này cho phép đánh giá hiệu quả mô hình với nhiều giá trị C khác nhau, từ đó tìm ra sự cân bằng hợp lý giữa độ chính xác và khả năng dự đoán trên dữ liệu mới."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e1b7b9-2c09-4e61-9974-31c23eb0693f",
   "metadata": {},
   "source": [
    "#### ***Viết đoạn code mẫu bằng Python (sử dụng Scikit-learn) để xây dựng một mô hình SVM cho bài toán phân loại không? Hãy mô tả các bước thực hiện***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093270e-b8b9-4b10-9f88-032b31082086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f90126e-bc17-4129-8102-fee1bbd5063c",
   "metadata": {},
   "source": [
    "#### ***Hàm nào trong Scikit-learn để chuẩn hóa dữ liệu (scaling) trước khi áp dụng SVM? Tại sao bước này quan trọng?***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b19033-174d-43d7-b8b1-7d06d01f4d92",
   "metadata": {},
   "source": [
    "### **2.3. GIẢI THUẬT 3: BAYES NGÂY THƠ (NAÏVE BAYES)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084503c-5154-4f17-8c50-b05ed959a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
